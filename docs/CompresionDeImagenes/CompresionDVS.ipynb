{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compresión de imagenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicaciones de la descomposición en valores singulares.\n",
    "\n",
    "La descomposición en valores singulares de una matriz $A$ es la factorización de $A$ como el producto de dos o más matrices de tal forma que sus factores satisfagan ciertas propiedades deseadas, según el problema a resolver. <!--En particualr es común que se busque una factorización cómo la siguiente $A=UDV^T$ donde las columnas de $U$ y $V$ son ortonormales, y la matriz D es Diagonal con entradas positivas y relaes <!--(como Wu-Tang Clan)-->\n",
    "\n",
    "Es usual procede factorizando la matriz en el producto de matrices triangulares, sea el caso de $LU$ y sus variantes ($LDU,LUP,LL^T$) para resolver ecuaciones lineales como: $Ax=b$ dónde $A\\in\\mathbb{R}^{n\\times n}$ es una matriz cuadrada  y $b\\in\\mathbb{R}^n$ es un vector. Diremos que $L$ es una matriz triangular inferior con $1$s en la diagonal y $U$ es una matriz triangular superior.\n",
    "\n",
    "Una vez que podemos escribir $A$ como $A=LU$, podemos ver el problema origianl como $LUx=b$, lo cual es equivalente al sistema de ecuaciones $Ly=b$ y $Ux=y$. La solución al sistema resulta sencilla, ya que las matrices $U$ y $L$ son triangulares. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuestiones computacionales.\n",
    "\n",
    "### Tiempo de computo y espacio de alamcenamiento de métodos tipo DVS\n",
    "\n",
    "La solución del sistema debe ser rápida, dado que $U$ y $L$ son matrices triangulares, se tiene pues que el costo computacional de este tipo de problemas es $O(n^2)$, lo que implica que el tiempo de computo es del tipo $T(n)=4n^2 − 2n + 2$.\n",
    "\n",
    "\"Las matrices $ L $ y $ U $ son esencialmente \"subproductos\" de la eliminación gaussiana, donde - $ L $ almacena los pasos del proceso de eliminación y $ U $ almacena la matriz resultante después de la eliminación. Así que para resolver $ Ax = b $ usando la eliminación gaussiana se necesitan $ \\frac {2} {3} n ^ 3 $ flops, mientras crea $ A = LU $, y resolver los dos sistemas triangulares resulta en $ \\frac {2} { 3} n ^ 3 + O (n ^ 2) $ flops que tienen la misma magnitud, por lo que no está claro por qué este esfuerzo adicional marcaría la diferencia. La ganancia que obtenemos es más clara si la tarea consiste en resolver muchas ecuaciones lineales que tienen la misma matriz $ A $ y sólo el lado derecho $ b $ cambia. En este caso, el procedimiento de eliminación, es decir, la creación de la $ LU $ -decomposición debe hacerse sólo una vez. Otra buena aplicación de esta descomposición es calcular el determinante de $ A $. Debido a las formas de $ L $ y $ U $ $$\\det(A) = \\det(LU) = \\det(L)\\cdot\\det(U) = \\det(U) = \\prod_{i=1}^nu_{ii},$$  Así que $ \\ det (A) $ se puede calcular en $ O (n ^ 3) $ flops también (en lugar de usar la regla de Cramer que tiene $ O (n \\cdot n!) $ Complejidad asintótica que lo hace prácticamente inútil).\"\n",
    "\n",
    "--INSERTAR Comparison_computational_complexity.svg--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descomponiendo una matriz arbitraria\n",
    "\n",
    "Sea $ A \\in \\mathbb {R} ^ {n \\times m} $ una matriz arbitraria (no necesariamente cuadrada). También puede tener valores complejos. Tomando en cuenta sólo casos con valores reales, podemos decir que existen las matrices $ U \\in \\mathbb {R} ^ {n \\times n} $, $ D \\in \\mathbb {R} ^ {n \\ \\times m} $ y $ V \\in \\mathbb {R} ^ {M \\times m} $, tales que \n",
    "$$ A = UDV ^ *, $$ \n",
    "\n",
    "donde $ U $ y $ V $ son matrices unitarias, es decir $U^*U = UU^* = I_n$ y $ V ^ * V = VV ^ * = I_m $ y $ D $ es una matriz diagonal, es decir, es tal que $ d_ {ij} = 0 $ si $ i \\ne j $. La operación estelar significa transposición conjugada, que es $ (V ^ *) _ {ij} = \\overline V_ {ji} $, pero como ahora estamos tratando con matrices reales, esto es lo mismo que la transposición de la matriz. Los elementos diagonales en $ D $ son números no negativos, en orden decreciente:  $d_{ii} = \\sigma_i$, $\\sigma_1\\geq\\sigma_2\\geq\\ldots\\geq\\sigma_r > \\sigma_{r+1} = \\ldots = \\sigma_{\\min(n,m)} = 0$, donde $ r $ es el rango de la matriz $ A $. Estos $ \\sigma $ valores en la diagonal de $ D $ se llaman los valores singulares de $ A $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximaciones de \"bajo rango\" de $ A $\n",
    "\n",
    "Sea $ k \\in \\mathbb {N} $ un número natural dado, donde  $k\\leq\\text{rank}(A)\\leq\\min\\{n, m\\}$. Lo que buscamos es una matriz $ A_k $ con $ \\ text {rank} (A_k) = k $ que es la mejor aproximación de $ A $ entre las matrices que tienen rango igual a $ k $. Para formular el problema de aproximación de rango bajo, queremos resolver el siguiente problema de minimización: $$ \\left|\\left| A - B \\right|\\right|_F \\to \\min !\\qquad \\mbox{ suejto a }\\quad B\\in\\mathbb{R}^{n\\times m}, \\ \\text{rank}(B) = k. $$ Aquí $ \\left | \\left | X \\right | \\right | _F $ denota la norma de Frobenius de una matriz $ X $ que es el raíz cuadrada de la suma de cuadrados de los elementos de $ X $.\n",
    "$${\\displaystyle \\|A\\|_{F}={\\sqrt {\\sum _{i=1}^{m}\\sum _{j=1}^{n}|a_{ij}|^{2}}}={\\sqrt {\\operatorname {trace} (A^{{}^{*}}A)}}={\\sqrt {\\sum _{i=1}^{\\min\\{m,\\,n\\}}\\sigma _{i}^{2}}}}$$\n",
    "\n",
    "La solución de este problema puede obtenerse de la descomposición SVD de $ A $. Si $ A = UDV ^ T $, entonces mantenemos los primeros $ k $ valores en $ D $ como es y establecemos los valores singulares posteriores a cero. Denotemos la matriz diagonal resultante por $ D_k $. Es fácil ver que sólo tenemos que mantener las primeras $ k $ columnas de $ U $ y las primeras $ k $ filas de $ V $, ya que sus otras columnas serían multiplicadas por ceros de todos modos. En resumen, la matriz $ A_k: = U_kD_kV_k ^ * $ es la matriz más cercana a $ A $ (en la norma de Frobenius) con rango $ k $, donde $ U_k $ y $ V_k $ consisten en las primeras $ k $ columnas y Filas de $ U $ y $ V $, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compresión de imagenes usando DVS\n",
    "\n",
    "Las imágenes se representan en una matriz rectangular donde cada elemento corresponde al valor de escala de grises para ese píxel. Para las imágenes a color tenemos una matriz $ 3 $-dimensional de tamaño $ n \\times m \\times 3 $, donde $ n $ y $ m $ representan el número de píxeles verticalmente y horizontalmente, respectivamente, y para cada píxel almacenamos la intensidad Para los colores rojo, verde y azul.\n",
    "\n",
    "Lo que vamos a hacer es repetir el procedimiento de aproximación de rango inferior arriba en una matriz más grande, es decir, creamos la aproximación de rango bajo de una matriz que representa una imagen para cada color por separado. La matriz resultante $ 3 $-dimensional será una buena aproximación de la imagen original, como veremos pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen original\n",
    "\n",
    "--Insertar test.jpg--\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen comprimida K=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen comprimida K=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagen comprimida K=200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
